{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.0' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/adam/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# === Production configuration, logging, and utilities (top cell) ===\n",
        "import os, logging, json, random, platform, gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "CONFIG = {\n",
        "    'random_seed': 42,\n",
        "    'oof_pred_vol_halflife': 10,     # days for EWMA pred volatility\n",
        "    'realized_vol_halflife': 60,     # days for EWMA realized volatility\n",
        "    'vol_cap_multiple': 1.2,         # cap strategy vol to X * market vol\n",
        "    'risk_k_grid': [0.4, 0.6, 0.8, 1.0, 1.2, 1.4],\n",
        "'neutral_band_grid': [0.0, 0.02, 0.05, 0.10, 0.15, 0.20],  # deadband in z-score units\n",
        "    'artifacts_dir': 'artifacts',\n",
        "    'transaction_cost_bps': 1.0,   # one-way cost in basis points, applied on allocation change\n",
        "    # Feature engineering controls (kept moderate to avoid blow-up)\n",
        "    'feature_lag_list': [1, 5, 10, 20],\n",
        "    'rolling_windows': [5, 20, 60],\n",
        "    'ewma_halflives_feat': [10, 60],\n",
        "    'roc_windows': [5, 20],\n",
        "    'zscore_windows': [20, 60],\n",
        "    # Seasonality feature controls\n",
        "    'use_date_splits': True,\n",
        "    'date_min_train_days': 252,\n",
        "    'date_val_days': 120,\n",
        "    'seasonality_week_period': 5,\n",
        "    'seasonality_month_period': 21,\n",
        "    'seasonality_quarter_period': 63,\n",
        "    # CV and calibration controls\n",
        "    'cv_n_splits': 5,\n",
        "    'fold_embargo_days': 5,\n",
        "    'annualization': 252,\n",
        "    # Training/Ensembling controls\n",
        "    'time_decay_halflife_days': 180,     # sample weight half-life for recent data emphasis\n",
        "    'eval_correlation': True,            # log corr(pred, target) per fold\n",
        "    'final_topk_features': 150,          # None or int: use top-K features by gain\n",
        "    'final_ensemble_size': 5,            # N_FINAL for stability\n",
        "    'final_ensemble_size_xgb': 3,\n",
        "    'final_ensemble_size_cat': 3,\n",
        "    'use_sharpe_fobj': False,            # experimental custom objective\n",
        "    'use_corr_feval': True,              # report corr in LightGBM\n",
        "    'stability_filter_min_folds': 0,     # 0 disables stability filtering\n",
        "    'use_stacking_meta_learner': False,  # enable later when stacking is added\n",
        "    # Advanced controls\n",
        "    'enable_optuna': False,\n",
        "    'optuna_trials': 20,\n",
        "    'optuna_timeout_min': 20,\n",
        "    'optuna_folds': 1,                   # broaden later for stability\n",
        "    'optuna_stability_penalty': 0.0,\n",
        "    'enable_xgboost': True,\n",
        "    'enable_catboost': True,\n",
        "    'ensemble_weighting': 'sharpe',      # 'sharpe' or 'equal'\n",
        "    'downcast_numeric': True,\n",
        "    'memory_aggressive_gc': True,\n",
        "}\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def aggressive_gc(note: str = \"\"):\n",
        "    if CONFIG.get('memory_aggressive_gc', False):\n",
        "        try:\n",
        "            gc.collect()\n",
        "            if hasattr(np, 'memmap'):\n",
        "                pass\n",
        "            LOGGER.info(f'GC triggered {\"(\"+note+\")\" if note else \"\"}')\n",
        "        except Exception as _:\n",
        "            pass\n",
        "\n",
        "ensure_dir(CONFIG['artifacts_dir'])\n",
        "\n",
        "\n",
        "def get_logger(name: str = 'hull_prod', level: int = logging.INFO, log_path: str | None = None) -> logging.Logger:\n",
        "    logger = logging.getLogger(name)\n",
        "    if not logger.handlers:\n",
        "        logger.setLevel(level)\n",
        "        ch = logging.StreamHandler()\n",
        "        ch.setLevel(level)\n",
        "        fmt = logging.Formatter('[%(asctime)s] %(levelname)s:%(name)s: %(message)s')\n",
        "        ch.setFormatter(fmt)\n",
        "        logger.addHandler(ch)\n",
        "        if log_path:\n",
        "            fh = logging.FileHandler(log_path)\n",
        "            fh.setLevel(level)\n",
        "            fh.setFormatter(fmt)\n",
        "            logger.addHandler(fh)\n",
        "    return logger\n",
        "\n",
        "LOGGER = get_logger(log_path=os.path.join(CONFIG['artifacts_dir'], 'run.log'))\n",
        "\n",
        "# Deterministic seeds\n",
        "os.environ['PYTHONHASHSEED'] = str(CONFIG['random_seed'])\n",
        "random.seed(CONFIG['random_seed'])\n",
        "np.random.seed(CONFIG['random_seed'])\n",
        "\n",
        "# Environment info (LightGBM version may be unavailable here; best-effort)\n",
        "try:\n",
        "    import lightgbm as lgb  # for version logging only\n",
        "    lgb_ver = lgb.__version__\n",
        "except Exception:\n",
        "    lgb_ver = 'N/A'\n",
        "\n",
        "LOGGER.info('Environment info:')\n",
        "LOGGER.info(f\"Python: {platform.python_version()}\")\n",
        "LOGGER.info(f\"NumPy: {np.__version__}, Pandas: {pd.__version__}\")\n",
        "LOGGER.info(f\"LightGBM: {lgb_ver}\")\n",
        "LOGGER.info('Production utilities configured.')\n",
        "\n",
        "# === Volatility and Sharpe utilities (reusable) ===\n",
        "\n",
        "def ewma_std(series: pd.Series, halflife: int) -> pd.Series:\n",
        "    if series.isna().all():\n",
        "        return pd.Series(index=series.index, dtype=float)\n",
        "    v = series.ewm(halflife=halflife, min_periods=max(5, halflife//2)).var()\n",
        "    return np.sqrt(v).fillna(method='bfill').fillna(method='ffill')\n",
        "\n",
        "\n",
        "def annualized_sharpe(returns: pd.Series, days_per_year: int | None = None) -> float:\n",
        "    dpy = int(CONFIG.get('annualization', 252) if days_per_year is None else days_per_year)\n",
        "    mu = returns.mean() * dpy\n",
        "    sd = returns.std() * np.sqrt(dpy)\n",
        "    return float(mu / (sd + 1e-9))\n",
        "\n",
        "\n",
        "def fast_corr(y_true: pd.Series | np.ndarray, y_pred: pd.Series | np.ndarray) -> float:\n",
        "    a = np.asarray(y_true, dtype=float)\n",
        "    b = np.asarray(y_pred, dtype=float)\n",
        "    if a.size < 3 or b.size < 3:\n",
        "        return float('nan')\n",
        "    a = a - a.mean()\n",
        "    b = b - b.mean()\n",
        "    denom = (np.sqrt((a * a).sum()) * np.sqrt((b * b).sum())) + 1e-12\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return float((a * b).sum() / denom)\n",
        "\n",
        "\n",
        "def make_time_decay_weights(n: int, halflife_days: int | None) -> np.ndarray:\n",
        "    if halflife_days is None or halflife_days <= 0:\n",
        "        return np.ones(n, dtype=float)\n",
        "    idx = np.arange(n, dtype=float)\n",
        "    dist = (n - 1) - idx\n",
        "    lam = np.log(2.0) / max(1.0, float(halflife_days))\n",
        "    w = np.exp(-lam * dist)\n",
        "    w = np.clip(w, 1e-6, None)\n",
        "    return w.astype(float)\n",
        "\n",
        "# Inverse-volatility modifier for sample weights (stabilize corr)\n",
        "def apply_inverse_vol_weights(base_weights: np.ndarray, realized: pd.Series | None, halflife: int) -> np.ndarray:\n",
        "    if realized is None or realized.isna().all():\n",
        "        return base_weights\n",
        "    vol = ewma_std(realized.fillna(0.0), halflife)\n",
        "    vol = vol.replace(0, np.nan).fillna(vol.median() if vol.notna().any() else 1.0)\n",
        "    inv = 1.0 / (vol.values + 1e-6)\n",
        "    inv = inv / np.nanmedian(inv)\n",
        "    w = base_weights * inv[:len(base_weights)]\n",
        "    return w.astype(float)\n",
        "\n",
        "\n",
        "def corr_feval(preds: np.ndarray, train_data) -> tuple[str, float, bool]:\n",
        "    labels = train_data.get_label()\n",
        "    c = fast_corr(labels, preds)\n",
        "    # higher is better\n",
        "    return 'corr', float(c if np.isfinite(c) else 0.0), True\n",
        "\n",
        "\n",
        "def sharpe_loss(preds: np.ndarray, train_data):\n",
        "    # Experimental surrogate: minimize negative Sharpe of returns = labels * preds\n",
        "    y = train_data.get_label().astype(float)\n",
        "    r = y * preds\n",
        "    mu = r.mean()\n",
        "    sd = r.std() + 1e-9\n",
        "    # d/dpred of mu = y/len, but LightGBM expects gradients per-sample without 1/n scaling\n",
        "    # Here we apply average later by dividing by len(y)\n",
        "    # d/dpred of sd = (r - mu) * y / (sd * len)\n",
        "    grad = - ( (y / sd) - ((r - mu) * mu * y) / (sd**3) )\n",
        "    grad = grad / max(len(y), 1)\n",
        "    # Use small positive hessian for stability\n",
        "    hess = np.full_like(preds, 1e-6, dtype=float)\n",
        "    return grad, hess\n",
        "\n",
        "\n",
        "# Grid-search calibration of allocation mapping parameters (k, band)\n",
        "# using only the provided prediction/realized series\n",
        "\n",
        "def calibrate_k_and_band(oof_pred: pd.Series,\n",
        "                         realized: pd.Series,\n",
        "                         pred_halflife: int,\n",
        "                         realized_halflife: int,\n",
        "                         k_grid: list,\n",
        "                         band_grid: list,\n",
        "                         vol_cap_multiple: float = 1.2,\n",
        "                         annualization: int | None = None,\n",
        "                         turnover_bps: float | None = None) -> dict:\n",
        "    assert len(oof_pred) == len(realized)\n",
        "    oof_pred = oof_pred.astype(float)\n",
        "    realized = realized.astype(float)\n",
        "\n",
        "    pred_sigma = ewma_std(oof_pred, pred_halflife)\n",
        "    mkt_vol = ewma_std(realized, realized_halflife)\n",
        "\n",
        "    dpy = int(CONFIG.get('annualization', 252) if annualization is None else annualization)\n",
        "\n",
        "    best = {'score': -np.inf, 'k': None, 'band': None}\n",
        "\n",
        "    for k in k_grid:\n",
        "        z = oof_pred / (pred_sigma.replace(0, np.nan).fillna(1e-6))\n",
        "        for band in band_grid:\n",
        "            z_band = z.where(z.abs() >= band, 0.0)\n",
        "            alloc = (1.0 + k * z_band).clip(0.0, 2.0)\n",
        "            strat_ret = alloc * realized\n",
        "            strat_vol = ewma_std(strat_ret, realized_halflife)\n",
        "            cap = (vol_cap_multiple * mkt_vol) / (strat_vol.replace(0, np.nan).fillna(1e-6))\n",
        "            cap = cap.clip(0.0, 2.0)\n",
        "            alloc_scaled = (1.0 + (alloc - 1.0) * cap).clip(0.0, 2.0)\n",
        "            strat_ret_scaled = alloc_scaled * realized\n",
        "            # Apply turnover penalty if requested\n",
        "            if turnover_bps is not None and turnover_bps > 0:\n",
        "                alloc_change = alloc_scaled.diff().abs().fillna(0.0)\n",
        "                tc = (turnover_bps / 10000.0) * alloc_change\n",
        "                strat_ret_scaled = strat_ret_scaled - tc\n",
        "            mu = strat_ret_scaled.mean() * dpy\n",
        "            sd = strat_ret_scaled.std() * np.sqrt(dpy)\n",
        "            score = (mu / (sd + 1e-9)) if sd > 0 else -np.inf\n",
        "            if score > best['score']:\n",
        "                best = {'score': float(score), 'k': float(k), 'band': float(band)}\n",
        "    return best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Competition tuning switches and GPU auto-detection ===\n",
        "# Flip on deeper Optuna, stacking, and larger ensemble sizes within runtime safety\n",
        "CONFIG.update({\n",
        "    'enable_optuna': True,         # re-enable tuning with safe budget\n",
        "    'optuna_folds': 2,\n",
        "    'optuna_trials': 120,\n",
        "    'optuna_timeout_min': 60,\n",
        "    'use_stacking_meta_learner': True,\n",
        "    'final_ensemble_size': 15,     # more LGB seeds\n",
        "    'final_ensemble_size_xgb': 7,  # more XGB seeds\n",
        "    'final_ensemble_size_cat': 0,  # keep CAT off\n",
        "    'final_topk_features': 100,    # tighter feature set for stability\n",
        "    'stability_filter_min_folds': 3,\n",
        "    'fold_embargo_days': 10,       # more embargo for cleaner CV\n",
        "    'ensemble_weighting': 'nnls',  # correlation-oriented blending\n",
        "    'pred_smooth_halflife': 5,     # EWMA smoothing before calibration\n",
        "    'use_gpu_if_available': False, # force CPU\n",
        "    'enable_catboost': False,      # disable CatBoost to avoid runtime/driver issues\n",
        "})\n",
        "\n",
        "# GPU detection (Kaggle-friendly, no internet)\n",
        "GPU_AVAILABLE = False\n",
        "try:\n",
        "    import os\n",
        "    # Common hints in Kaggle\n",
        "    if os.environ.get('NVIDIA_VISIBLE_DEVICES') not in (None, '', 'none'):\n",
        "        GPU_AVAILABLE = True\n",
        "    if os.environ.get('KAGGLE_ACCELERATOR_TYPE', '').upper().find('GPU') >= 0:\n",
        "        GPU_AVAILABLE = True\n",
        "    # Linux NVIDIA presence fallback (ignored on Windows)\n",
        "    if os.name == 'posix' and not GPU_AVAILABLE:\n",
        "        GPU_AVAILABLE = os.path.exists('/proc/driver/nvidia/version')\n",
        "except Exception:\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "LOGGER.info(f\"GPU available: {GPU_AVAILABLE}\")\n",
        "\n",
        "# CPU fallback to keep within time limits\n",
        "if not GPU_AVAILABLE:\n",
        "    CONFIG.update({\n",
        "        'final_ensemble_size': min(int(CONFIG.get('final_ensemble_size', 9)), 5),\n",
        "        'final_ensemble_size_xgb': min(int(CONFIG.get('final_ensemble_size_xgb', 5)), 2),\n",
        "        'final_ensemble_size_cat': min(int(CONFIG.get('final_ensemble_size_cat', 5)), 2),\n",
        "        'optuna_trials': min(int(CONFIG.get('optuna_trials', 300)), 120),\n",
        "        'optuna_timeout_min': min(int(CONFIG.get('optuna_timeout_min', 120)), 60),\n",
        "        'cv_n_splits': min(int(CONFIG.get('cv_n_splits', 5)), 4),\n",
        "    })\n",
        "    LOGGER.info('[CPU] Reduced ensemble sizes and HPO budget for runtime safety')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Hull Tactical — Market Prediction: Strong baseline notebook\n",
        "What this notebook does\n",
        "- Loads local train/test (no internet)\n",
        "- Time-series-safe preprocessing and light feature engineering\n",
        "- Walk-forward (expanding window) cross-validation with LightGBM\n",
        "- Volatility-aware allocation mapping in [0, 2] with a practical risk cap\n",
        "- Diagnostics: CV RMSE, feature importances, simple backtest and Sharpe stats\n",
        "- Trains final ensemble and writes submission.csv with columns ['date_id','allocation']\n",
        "\n",
        "Notes & cautions\n",
        "- This is a reproducible, efficient baseline designed to run < 1 hour in Kaggle.\n",
        "- To reach top leaderboard: engineer better features (macro, cross-asset, seasonalities),\n",
        "  robust stacking/ensembling, volatility forecasting, and careful walk-forward tuning.\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "SEED = CONFIG['random_seed'] if 'CONFIG' in globals() else 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "TRAIN_PATH = 'train.csv'\n",
        "TEST_PATH  = 'test.csv'\n",
        "EVAL_DIR   = 'kaggle_evaluation'  # present in competition env; not required here\n",
        "\n",
        "# Auto-detect paths on Kaggle\n",
        "try:\n",
        "    import os\n",
        "    if os.path.exists('/kaggle/input'):\n",
        "        roots = [p for p in os.listdir('/kaggle/input') if os.path.isdir(os.path.join('/kaggle/input', p))]\n",
        "        for r in roots:\n",
        "            p_train = os.path.join('/kaggle/input', r, 'train.csv')\n",
        "            p_test  = os.path.join('/kaggle/input', r, 'test.csv')\n",
        "            if os.path.exists(p_train) and os.path.exists(p_test):\n",
        "                TRAIN_PATH = p_train\n",
        "                TEST_PATH = p_test\n",
        "                break\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# 1) Load data\n",
        "LOGGER.info(f'Loading data from: {TRAIN_PATH} | {TEST_PATH}')\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test  = pd.read_csv(TEST_PATH)\n",
        "\n",
        "# Sort by time to ensure proper temporal order\n",
        "if 'date_id' in train.columns:\n",
        "    train = train.sort_values('date_id').reset_index(drop=True)\n",
        "if 'date_id' in test.columns:\n",
        "    test = test.sort_values('date_id').reset_index(drop=True)\n",
        "\n",
        "LOGGER.info(f'Train shape: {train.shape}')\n",
        "LOGGER.info(f'Test  shape: {test.shape}')\n",
        "\n",
        "# 2) Target selection (excess returns preferred). We keep a realized return series for backtest if possible.\n",
        "# Priority: use 'market_forward_excess_returns' if provided; otherwise compute from 'forward_returns' - 'risk_free_rate'.\n",
        "if 'market_forward_excess_returns' in train.columns:\n",
        "    TARGET = 'market_forward_excess_returns'\n",
        "    LOGGER.info('Using TARGET = market_forward_excess_returns')\n",
        "else:\n",
        "    if 'forward_returns' in train.columns and 'risk_free_rate' in train.columns:\n",
        "        train['market_forward_excess_returns'] = train['forward_returns'] - train['risk_free_rate']\n",
        "        TARGET = 'market_forward_excess_returns'\n",
        "        LOGGER.info('Computed TARGET = forward_returns - risk_free_rate')\n",
        "    else:\n",
        "        raise ValueError('Target not found: need market_forward_excess_returns or (forward_returns and risk_free_rate).')\n",
        "\n",
        "# Realized returns for toy backtest\n",
        "if 'forward_returns' in train.columns:\n",
        "    realized_returns = train['forward_returns'].copy()\n",
        "elif 'market_forward_excess_returns' in train.columns and 'risk_free_rate' in train.columns:\n",
        "    realized_returns = train['market_forward_excess_returns'] + train['risk_free_rate']\n",
        "elif 'market_forward_excess_returns' in train.columns:\n",
        "    realized_returns = train['market_forward_excess_returns'].copy()\n",
        "else:\n",
        "    realized_returns = None\n",
        "\n",
        "# 3) Feature selection and preprocessing\n",
        "# Use only numeric features, excluding identifiers and targets\n",
        "exclude_cols = {'date_id', TARGET, 'forward_returns', 'risk_free_rate'}\n",
        "num_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "feature_cols = [c for c in num_cols if c not in exclude_cols]\n",
        "LOGGER.info(f'Initial numeric feature count: {len(feature_cols)}')\n",
        "\n",
        "# Concatenate train+test in time order for leak-free lag/rolling creation (test follows train)\n",
        "DF = pd.concat([\n",
        "    train[feature_cols + (['date_id'] if 'date_id' in train.columns else [])],\n",
        "    test[feature_cols + (['date_id'] if 'date_id' in test.columns else [])]\n",
        "], axis=0, ignore_index=True)\n",
        "\n",
        "# Replace infs and fill base NaNs using train-only medians\n",
        "train_rows = train.shape[0]\n",
        "for col in feature_cols:\n",
        "    DF[col] = DF[col].replace([np.inf, -np.inf], np.nan)\n",
        "base_medians = train[feature_cols].median()\n",
        "DF[feature_cols] = DF[feature_cols].fillna(base_medians)\n",
        "\n",
        "# Light, fast feature engineering: 1-lag and 5-day rolling mean (shifted) per column\n",
        "lag_cols = []\n",
        "for col in feature_cols:\n",
        "    lag1 = f'{col}_lag1'\n",
        "    rmean5 = f'{col}_rmean5'\n",
        "    DF[lag1] = DF[col].shift(1)\n",
        "    DF[rmean5] = DF[col].rolling(window=5, min_periods=1).mean().shift(1)\n",
        "    lag_cols.extend([lag1, rmean5])\n",
        "\n",
        "# Fill lag feature NaNs using train-only medians to avoid leakage\n",
        "lag_medians = DF.iloc[:train_rows][lag_cols].median()\n",
        "DF[lag_cols] = DF[lag_cols].fillna(lag_medians)\n",
        "\n",
        "# Re-split into engineered train/test\n",
        "train_fe = DF.iloc[:train_rows].copy()\n",
        "test_fe  = DF.iloc[train_rows:].copy()\n",
        "\n",
        "# Restore target and date_id\n",
        "train_fe[TARGET] = train[TARGET].values\n",
        "if 'date_id' in train.columns:\n",
        "    train_fe['date_id'] = train['date_id'].values\n",
        "if 'date_id' in test.columns:\n",
        "    test_fe['date_id'] = test['date_id'].values\n",
        "\n",
        "# Final feature list\n",
        "features = [c for c in train_fe.columns if c not in ['date_id', TARGET]]\n",
        "\n",
        "# Cast features to float32 for efficiency and reduce memory\n",
        "if bool(CONFIG.get('downcast_numeric', False)):\n",
        "    def downcast_df(df):\n",
        "        for c in df.select_dtypes(include=[np.number]).columns:\n",
        "            col = df[c]\n",
        "            if pd.api.types.is_float_dtype(col):\n",
        "                df[c] = pd.to_numeric(col, downcast='float')\n",
        "            elif pd.api.types.is_integer_dtype(col):\n",
        "                df[c] = pd.to_numeric(col, downcast='integer')\n",
        "        return df\n",
        "    train_fe[features] = downcast_df(train_fe[features])\n",
        "    test_fe[features] = downcast_df(test_fe[features])\n",
        "else:\n",
        "    train_fe[features] = train_fe[features].astype('float32')\n",
        "    test_fe[features] = test_fe[features].astype('float32')\n",
        "\n",
        "aggressive_gc('post-basic-feature-casting')\n",
        "\n",
        "LOGGER.info(f'Final features count: {len(features)}')\n",
        "\n",
        "# Quick EDA prints\n",
        "LOGGER.info('Basic EDA:')\n",
        "LOGGER.info(f\"Target mean/std: {train_fe[TARGET].mean():.6f} {train_fe[TARGET].std():.6f}\")\n",
        "missing_rate = train[feature_cols].isna().mean().mean()\n",
        "LOGGER.info(f'Average missing rate (pre-impute) over base features: {missing_rate:.4f}')\n",
        "\n",
        "# 4) Time-series CV: expanding window / walk-forward\n",
        "\n",
        "def expanding_walk_forward_splits(n_samples: int,\n",
        "                                  n_splits: int = 5,\n",
        "                                  min_train_ratio: float = 0.6,\n",
        "                                  val_size_ratio: float = 0.1,\n",
        "                                  min_train: int = 252,\n",
        "                                  min_val: int = 120):\n",
        "    min_train_size = max(int(n_samples * min_train_ratio), min_train)\n",
        "    val_size = max(int(n_samples * val_size_ratio), min_val)\n",
        "    if min_train_size + val_size >= n_samples:\n",
        "        # fallback to ensure at least one fold\n",
        "        min_train_size = max(min_train, n_samples - 2 * min_val)\n",
        "        val_size = min_val\n",
        "    starts = np.linspace(min_train_size, n_samples - val_size, num=n_splits, dtype=int)\n",
        "    seen = set()\n",
        "    for s in starts:\n",
        "        if s in seen:\n",
        "            continue\n",
        "        seen.add(s)\n",
        "        tr_idx = np.arange(0, s)\n",
        "        val_end = min(s + val_size, n_samples)\n",
        "        val_idx = np.arange(s, val_end)\n",
        "        if len(val_idx) > 0:\n",
        "            yield tr_idx, val_idx\n",
        "\n",
        "n_samples = train_fe.shape[0]\n",
        "splits = list(expanding_walk_forward_splits(n_samples, n_splits=5))\n",
        "LOGGER.info(f'CV folds: {len(splits)}')\n",
        "\n",
        "# 5) Train LightGBM models on each fold\n",
        "lgb_params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',  # use RMSE for early stopping; corr only as feval\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.02,\n",
        "    'num_leaves': 64,\n",
        "    'max_depth': -1,\n",
        "    'min_data_in_leaf': 50,\n",
        "    'feature_fraction': 0.7,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'seed': SEED,\n",
        "    'bagging_seed': SEED + 1,\n",
        "    'feature_fraction_seed': SEED + 2,\n",
        "    'verbosity': -1,\n",
        "    'num_threads': -1,\n",
        "}\n",
        "\n",
        "# GPU hooks (if available)\n",
        "if bool(CONFIG.get('use_gpu_if_available', False)) and 'GPU_AVAILABLE' in globals() and GPU_AVAILABLE:\n",
        "    try:\n",
        "        lgb_params.update({'device_type': 'gpu'})\n",
        "        LOGGER.info('[GPU] Enabled LightGBM GPU')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "models = []\n",
        "val_scores = []\n",
        "feature_importance_df = pd.DataFrame()\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(splits):\n",
        "    X_tr = train_fe.iloc[tr_idx][features]\n",
        "    y_tr = train_fe.iloc[tr_idx][TARGET]\n",
        "    X_val = train_fe.iloc[val_idx][features]\n",
        "    y_val = train_fe.iloc[val_idx][TARGET]\n",
        "\n",
        "    LOGGER.info(f'Fold {fold+1}/{len(splits)} — train {len(tr_idx)} val {len(val_idx)}')\n",
        "    dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
        "    dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "    try:\n",
        "        bst = lgb.train(\n",
        "            params=lgb_params,\n",
        "            train_set=dtrain,\n",
        "            num_boost_round=2000,\n",
        "            valid_sets=[dtrain, dvalid],\n",
        "            valid_names=['train', 'valid'],\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
        "                lgb.log_evaluation(period=200),\n",
        "            ],\n",
        "            feval=corr_feval if bool(CONFIG.get('use_corr_feval', False)) else None,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        # Fallback to CPU if GPU device not available\n",
        "        if isinstance(e, Exception):\n",
        "            lgb_params.pop('device_type', None)\n",
        "            bst = lgb.train(\n",
        "                params=lgb_params,\n",
        "                train_set=dtrain,\n",
        "                num_boost_round=2000,\n",
        "                valid_sets=[dtrain, dvalid],\n",
        "                valid_names=['train', 'valid'],\n",
        "                callbacks=[\n",
        "                    lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
        "                    lgb.log_evaluation(period=200),\n",
        "                ],\n",
        "                feval=corr_feval if bool(CONFIG.get('use_corr_feval', False)) else None,\n",
        "            )\n",
        "\n",
        "    models.append(bst)\n",
        "\n",
        "    val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
        "    rmse = mean_squared_error(y_val, val_pred, squared=False)\n",
        "    LOGGER.info(f'Fold {fold+1} RMSE: {rmse:.6f}')\n",
        "    val_scores.append(rmse)\n",
        "\n",
        "    fi = pd.DataFrame({\n",
        "        'feature': features,\n",
        "        'importance': bst.feature_importance(importance_type='gain'),\n",
        "        'fold': fold,\n",
        "    })\n",
        "    feature_importance_df = pd.concat([feature_importance_df, fi], axis=0)\n",
        "\n",
        "    del X_tr, y_tr, X_val, y_val, dtrain, dvalid\n",
        "    gc.collect()\n",
        "\n",
        "LOGGER.info(f\"CV RMSE mean: {float(np.mean(val_scores)):.6f}\")\n",
        "LOGGER.info(f\"CV RMSE std : {float(np.std(val_scores)):.6f}\")\n",
        "\n",
        "# 6) OOF predictions for diagnostics/backtest\n",
        "oof = np.zeros(n_samples, dtype=float)\n",
        "counts = np.zeros(n_samples, dtype=float)\n",
        "for fold, (tr_idx, val_idx) in enumerate(splits):\n",
        "    bst = models[fold]\n",
        "    oof[val_idx] += bst.predict(train_fe.iloc[val_idx][features], num_iteration=bst.best_iteration)\n",
        "    counts[val_idx] += 1\n",
        "\n",
        "mask = counts > 0\n",
        "oof[mask] /= counts[mask]\n",
        "train_fe['pred'] = oof\n",
        "\n",
        "# Feature importances (mean over folds)\n",
        "fi_mean = (feature_importance_df.groupby('feature')['importance']\n",
        "           .mean().sort_values(ascending=False))\n",
        "LOGGER.info('Top 20 features by average gain:')\n",
        "LOGGER.info(f\"\\n{fi_mean.head(20)}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "fi_mean.head(20).sort_values().plot(kind='barh')\n",
        "plt.title('Top 20 feature importances (avg gain)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG['artifacts_dir'], 'feature_importances_top20.png'))\n",
        "plt.close()\n",
        "\n",
        "# 7) Simple volatility-aware mapping from predictions to allocation in [0, 2]\n",
        "# Estimate rolling prediction volatility to scale signals\n",
        "pred_sigma = pd.Series(train_fe['pred']).rolling(window=20, min_periods=5).std()\n",
        "pred_sigma = pred_sigma.fillna(pred_sigma.iloc[5:25].median() if pred_sigma.notna().any() else 1.0)\n",
        "\n",
        "risk_k = 0.8  # aggressiveness; tune in walk-forward if time allows\n",
        "alloc_oof = 1.0 + risk_k * (train_fe['pred'] / (pred_sigma.replace(0, np.nan).fillna(1e-6)))\n",
        "alloc_oof = alloc_oof.clip(0.0, 2.0)\n",
        "train_fe['alloc'] = alloc_oof\n",
        "\n",
        "# Enforce vol cap: realized strategy vol <= 1.2 * market vol (rolling)\n",
        "if realized_returns is not None:\n",
        "    train_fe['realized_returns'] = realized_returns.values\n",
        "    strat_ret_raw = train_fe['alloc'] * train_fe['realized_returns']\n",
        "    strat_vol_180 = strat_ret_raw.rolling(180, min_periods=30).std().fillna(strat_ret_raw.std())\n",
        "    market_vol_180 = train_fe['realized_returns'].rolling(180, min_periods=30).std().fillna(train_fe['realized_returns'].std())\n",
        "\n",
        "    scale = (1.2 * market_vol_180) / (strat_vol_180.replace(0, np.nan).fillna(1e-6))\n",
        "    scale = scale.clip(0.0, 2.0)\n",
        "    alloc_scaled = 1.0 + (train_fe['alloc'] - 1.0) * scale\n",
        "    train_fe['alloc_scaled'] = alloc_scaled.clip(0.0, 2.0)\n",
        "\n",
        "    # Simple transaction cost model: one-way cost on changes in allocation\n",
        "    tc = CONFIG.get('transaction_cost_bps', 0.0) / 10000.0\n",
        "    alloc_change_raw = train_fe['alloc'].diff().abs().fillna(0.0)\n",
        "    alloc_change_scaled = train_fe['alloc_scaled'].diff().abs().fillna(0.0)\n",
        "    tc_raw = tc * alloc_change_raw\n",
        "    tc_scaled = tc * alloc_change_scaled\n",
        "\n",
        "    # Backtest diagnostics (after costs)\n",
        "    train_fe['strat_ret_raw'] = strat_ret_raw - tc_raw\n",
        "    train_fe['strat_ret_scaled'] = (train_fe['alloc_scaled'] * train_fe['realized_returns']) - tc_scaled\n",
        "\n",
        "    train_fe['cum_sp500'] = (1.0 + train_fe['realized_returns']).cumprod()\n",
        "    train_fe['cum_raw']   = (1.0 + train_fe['strat_ret_raw']).cumprod()\n",
        "    train_fe['cum_scaled']= (1.0 + train_fe['strat_ret_scaled']).cumprod()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x_axis = train_fe['date_id'] if 'date_id' in train_fe.columns else np.arange(len(train_fe))\n",
        "    plt.plot(x_axis, train_fe['cum_sp500'], label='Market (buy-hold)')\n",
        "    plt.plot(x_axis, train_fe['cum_raw'], label='Strategy (raw)')\n",
        "    plt.plot(x_axis, train_fe['cum_scaled'], label='Strategy (vol scaled)')\n",
        "    plt.legend(); plt.title('Cumulative performance (toy backtest)'); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(CONFIG['artifacts_dir'], 'backtest_cumulative.png'))\n",
        "    plt.close()\n",
        "\n",
        "    def ann_sharpe(returns: pd.Series, days_per_year: int = 252):\n",
        "        mu = returns.mean() * days_per_year\n",
        "        sd = returns.std() * np.sqrt(days_per_year)\n",
        "        return float(mu / (sd + 1e-9))\n",
        "\n",
        "    LOGGER.info(f\"Ann Sharpe raw   (toy): {ann_sharpe(train_fe['strat_ret_raw'].fillna(0)):.4f}\")\n",
        "    LOGGER.info(f\"Ann Sharpe scaled(toy): {ann_sharpe(train_fe['strat_ret_scaled'].fillna(0)):.4f}\")\n",
        "    LOGGER.info(f\"Ann Sharpe market      : {ann_sharpe(train_fe['realized_returns'].fillna(0)):.4f}\")\n",
        "else:\n",
        "    LOGGER.warning('Skipping backtest: realized forward returns not available in train.')\n",
        "\n",
        "# 8) Train final ensemble on full train and predict test\n",
        "X_full = train_fe[features]\n",
        "y_full = train_fe[TARGET]\n",
        "\n",
        "avg_best_iter = int(np.clip(np.mean([m.best_iteration for m in models]), 200, 2000)) if len(models) else 1000\n",
        "LOGGER.info(f'Using avg_best_iter: {avg_best_iter}')\n",
        "\n",
        "final_models = []\n",
        "N_FINAL = int(CONFIG.get('final_ensemble_size', 3))  # config-driven ensemble size\n",
        "for i in range(N_FINAL):\n",
        "    params = lgb_params.copy()\n",
        "    params['seed'] = SEED + 13 * i\n",
        "    params['bagging_seed'] = SEED + 13 * i + 1\n",
        "    params['feature_fraction_seed'] = SEED + 13 * i + 2\n",
        "    w_full = make_time_decay_weights(len(X_full), int(CONFIG.get('time_decay_halflife_days', 0)))\n",
        "    dtrain = lgb.Dataset(X_full, label=y_full, weight=w_full)\n",
        "    try:\n",
        "        bst = lgb.train(params, dtrain, num_boost_round=avg_best_iter)\n",
        "    except Exception as e:\n",
        "        if isinstance(e, Exception):\n",
        "            params.pop('device_type', None)\n",
        "            bst = lgb.train(params, dtrain, num_boost_round=avg_best_iter)\n",
        "    final_models.append(bst)\n",
        "    del dtrain\n",
        "\n",
        "def rank_normalize(arr: np.ndarray) -> np.ndarray:\n",
        "    n = len(arr)\n",
        "    if n <= 1:\n",
        "        return np.zeros_like(arr, dtype=float)\n",
        "    ranks = np.argsort(np.argsort(arr))\n",
        "    return ranks.astype(float) / max(n - 1, 1)\n",
        "\n",
        "# Predict test with rank-based ensembling (often more robust for Sharpe-like metrics)\n",
        "X_test = test_fe[features]\n",
        "preds_list = []\n",
        "for bst in final_models:\n",
        "    # ensure a concrete iteration count\n",
        "    preds_list.append(bst.predict(X_test, num_iteration=avg_best_iter))\n",
        "\n",
        "# Average of ranks (0..1), then center roughly around 0 by subtracting 0.5 for mapping\n",
        "ranked_preds = np.stack([rank_normalize(p) for p in preds_list], axis=1).mean(axis=1)\n",
        "preds_test = ranked_preds - 0.5\n",
        "\n",
        "# Allocation mapping in [0, 2]\n",
        "# Use sigma estimated from recent prediction volatility on train as a proxy\n",
        "pred_sigma_train = pd.Series(train_fe['pred']).rolling(window=20, min_periods=5).std()\n",
        "if pred_sigma_train.notna().any():\n",
        "    sigma_est = float(pred_sigma_train.iloc[-50:].median()) if pred_sigma_train.iloc[-50:].notna().any() else float(pred_sigma_train.dropna().median())\n",
        "else:\n",
        "    sigma_est = float(train_fe['pred'].std() if 'pred' in train_fe else 1.0)\n",
        "\n",
        "k = 0.8  # same aggressiveness factor used above\n",
        "alloc_test = 1.0 + k * (preds_test / (sigma_est + 1e-9))\n",
        "alloc_test = np.clip(alloc_test, 0.0, 2.0)\n",
        "\n",
        "# Apply a global downscale if recent historical strategy vol would breach cap\n",
        "if realized_returns is not None:\n",
        "    hist_window = min(len(train_fe), 180)\n",
        "    hist_strat_vol = (train_fe['alloc'].iloc[-hist_window:] * train_fe['realized_returns'].iloc[-hist_window:]).std()\n",
        "    hist_mkt_vol   = train_fe['realized_returns'].iloc[-hist_window:].std()\n",
        "    scale_global = min(1.0, (CONFIG['vol_cap_multiple'] * hist_mkt_vol) / (hist_strat_vol + 1e-9)) if hist_strat_vol > 0 else 1.0\n",
        "    alloc_test = 1.0 + (alloc_test - 1.0) * scale_global\n",
        "    alloc_test = np.clip(alloc_test, 0.0, 2.0)\n",
        "\n",
        "# Do not write submission here; defer to calibrated step\n",
        "submission = pd.DataFrame({\n",
        "    'date_id': test_fe['date_id'].values if 'date_id' in test_fe.columns else np.arange(len(test_fe)),\n",
        "    'allocation': alloc_test.astype(float)\n",
        "})\n",
        "LOGGER.info(f\"Prepared preliminary submission with {submission.shape[0]} rows (not yet saved)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Enhanced feature engineering and nested Sharpe-centric CV ===\n",
        "import math\n",
        "\n",
        "LOGGER.info('Starting enhanced feature engineering and nested calibration CV...')\n",
        "\n",
        "# Rebuild engineered features in a modular way to avoid leakage\n",
        "\n",
        "def build_features(df: pd.DataFrame, base_cols: list[str]) -> tuple[pd.DataFrame, list[str]]:\n",
        "    out = df.copy()\n",
        "    created = []\n",
        "    lag_list = CONFIG.get('feature_lag_list', [1, 5, 10, 20])\n",
        "    roll_ws  = CONFIG.get('rolling_windows', [5, 20, 60])\n",
        "    ewm_halfs= CONFIG.get('ewma_halflives_feat', [10, 60])\n",
        "    roc_ws   = CONFIG.get('roc_windows', [5, 20])\n",
        "    z_ws     = CONFIG.get('zscore_windows', [20, 60])\n",
        "\n",
        "    # Seasonality proxies from date_id if available (shifted to avoid leakage)\n",
        "    if 'date_id' in out.columns:\n",
        "        d = out['date_id']\n",
        "        for P, name in [\n",
        "            (int(CONFIG.get('seasonality_week_period', 5)), 'wk'),\n",
        "            (int(CONFIG.get('seasonality_month_period', 21)), 'mo'),\n",
        "            (int(CONFIG.get('seasonality_quarter_period', 63)), 'qr'),\n",
        "        ]:\n",
        "            if P and P > 1:\n",
        "                out[f'sin_{name}'] = np.sin(2 * np.pi * (d % P) / P).shift(1)\n",
        "                out[f'cos_{name}'] = np.cos(2 * np.pi * (d % P) / P).shift(1)\n",
        "                created.extend([f'sin_{name}', f'cos_{name}'])\n",
        "\n",
        "    for col in base_cols:\n",
        "        s = out[col]\n",
        "        # Lags\n",
        "        for L in lag_list:\n",
        "            name = f'{col}_lag{L}'\n",
        "            out[name] = s.shift(L)\n",
        "            created.append(name)\n",
        "        # Rolling stats (shifted)\n",
        "        for W in roll_ws:\n",
        "            mean_name = f'{col}_rmean{W}'\n",
        "            std_name  = f'{col}_rstd{W}'\n",
        "            out[mean_name] = s.rolling(W, min_periods=1).mean().shift(1)\n",
        "            out[std_name]  = s.rolling(W, min_periods=1).std().shift(1)\n",
        "            created.extend([mean_name, std_name])\n",
        "        # EWMA std (shifted)\n",
        "        for H in ewm_halfs:\n",
        "            name = f'{col}_ewmstd_h{H}'\n",
        "            out[name] = s.ewm(halflife=H, min_periods=max(5, H//2)).std().shift(1)\n",
        "            created.append(name)\n",
        "        # Rate-of-change (shifted)\n",
        "        for W in roc_ws:\n",
        "            name = f'{col}_roc{W}'\n",
        "            out[name] = s.pct_change(W).shift(1)\n",
        "            created.append(name)\n",
        "        # Z-scores (shifted)\n",
        "        for W in z_ws:\n",
        "            mu = s.rolling(W, min_periods=1).mean().shift(1)\n",
        "            sd = s.rolling(W, min_periods=1).std().shift(1)\n",
        "            name = f'{col}_z{W}'\n",
        "            out[name] = (s.shift(1) - mu) / (sd.replace(0, np.nan) + 1e-6)\n",
        "            created.append(name)\n",
        "\n",
        "    # Vol-of-vol and rolling higher moments on the target proxy if present\n",
        "    target_proxy = None\n",
        "    if 'realized_returns' in out.columns:\n",
        "        target_proxy = out['realized_returns']\n",
        "    elif 'forward_returns' in out.columns:\n",
        "        target_proxy = out['forward_returns']\n",
        "    if target_proxy is not None:\n",
        "        for W in [20, 60, 120]:\n",
        "            name_vv = f'target_volofvol_{W}'\n",
        "            name_sk = f'target_skew_{W}'\n",
        "            name_ku = f'target_kurt_{W}'\n",
        "            r = target_proxy.shift(1)\n",
        "            out[name_vv] = r.rolling(W, min_periods=10).std().rolling(W, min_periods=10).std().shift(1)\n",
        "            out[name_sk] = r.rolling(W, min_periods=10).skew().shift(1)\n",
        "            out[name_ku] = r.rolling(W, min_periods=10).kurt().shift(1)\n",
        "            created.extend([name_vv, name_sk, name_ku])\n",
        "\n",
        "    return out, created\n",
        "\n",
        "# Build from scratch using the time-safe concat approach\n",
        "exclude_cols = {'date_id', TARGET, 'forward_returns', 'risk_free_rate'}\n",
        "num_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "base_feature_cols = [c for c in num_cols if c not in exclude_cols]\n",
        "\n",
        "DF_base = pd.concat([\n",
        "    train[base_feature_cols + (['date_id'] if 'date_id' in train.columns else [])],\n",
        "    test[base_feature_cols + (['date_id'] if 'date_id' in test.columns else [])]\n",
        "], axis=0, ignore_index=True)\n",
        "\n",
        "# Clean base features\n",
        "train_rows = train.shape[0]\n",
        "for col in base_feature_cols:\n",
        "    DF_base[col] = DF_base[col].replace([np.inf, -np.inf], np.nan)\n",
        "base_medians = train[base_feature_cols].median()\n",
        "DF_base[base_feature_cols] = DF_base[base_feature_cols].fillna(base_medians)\n",
        "\n",
        "# Build engineered features\n",
        "DF_eng, created_cols = build_features(DF_base, base_feature_cols)\n",
        "\n",
        "# Clean engineered features: replace infs, impute with train-only medians, and handle all-NaN cols\n",
        "DF_eng[created_cols] = DF_eng[created_cols].replace([np.inf, -np.inf], np.nan)\n",
        "eng_medians = DF_eng.iloc[:train_rows][created_cols].median()\n",
        "DF_eng[created_cols] = DF_eng[created_cols].fillna(eng_medians)\n",
        "# Drop engineered columns that remain all-NaN in train\n",
        "na_all_cols = [c for c in created_cols if DF_eng.iloc[:train_rows][c].isna().all()]\n",
        "if len(na_all_cols):\n",
        "    LOGGER.warning(f'Dropping {len(na_all_cols)} engineered features with all-NaN in train')\n",
        "    DF_eng.drop(columns=na_all_cols, inplace=True, errors='ignore')\n",
        "    created_cols = [c for c in created_cols if c not in na_all_cols]\n",
        "# Final safety fill for any residual NaNs\n",
        "DF_eng[created_cols] = DF_eng[created_cols].fillna(0.0)\n",
        "\n",
        "# Split back to train/test\n",
        "train_fe = DF_eng.iloc[:train_rows].copy()\n",
        "test_fe  = DF_eng.iloc[train_rows:].copy()\n",
        "\n",
        "# Restore target/date\n",
        "train_fe[TARGET] = train[TARGET].values\n",
        "if 'date_id' in train.columns:\n",
        "    train_fe['date_id'] = train['date_id'].values\n",
        "if 'date_id' in test.columns:\n",
        "    test_fe['date_id'] = test['date_id'].values\n",
        "\n",
        "# Realized returns for backtest\n",
        "if realized_returns is not None:\n",
        "    train_fe['realized_returns'] = realized_returns.values\n",
        "\n",
        "# Final feature set and types\n",
        "features = [c for c in train_fe.columns if c not in ['date_id', TARGET, 'realized_returns']]\n",
        "if bool(CONFIG.get('downcast_numeric', False)):\n",
        "    def downcast_df(df):\n",
        "        for c in df.select_dtypes(include=[np.number]).columns:\n",
        "            col = df[c]\n",
        "            if pd.api.types.is_float_dtype(col):\n",
        "                df[c] = pd.to_numeric(col, downcast='float')\n",
        "            elif pd.api.types.is_integer_dtype(col):\n",
        "                df[c] = pd.to_numeric(col, downcast='integer')\n",
        "        return df\n",
        "    train_fe[features] = downcast_df(train_fe[features])\n",
        "    test_fe[features] = downcast_df(test_fe[features])\n",
        "else:\n",
        "    train_fe[features] = train_fe[features].astype('float32')\n",
        "    test_fe[features] = test_fe[features].astype('float32')\n",
        "\n",
        "aggressive_gc('post-enhanced-feature-casting')\n",
        "\n",
        "LOGGER.info(f'Enhanced features count: {len(features)} (added {len(created_cols)} engineered)')\n",
        "\n",
        "# Expanding CV with optional embargo between train/val\n",
        "n_samples = train_fe.shape[0]\n",
        "splits = list(expanding_walk_forward_splits(n_samples, n_splits=int(CONFIG.get('cv_n_splits', 5))))\n",
        "LOGGER.info(f'CV folds (enhanced): {len(splits)}')\n",
        "\n",
        "# Optionally replace with date-aware splits (purged) based on unique date_id\n",
        "if bool(CONFIG.get('use_date_splits', False)) and 'date_id' in train_fe.columns:\n",
        "    def walk_forward_by_dates_local(df: pd.DataFrame,\n",
        "                                    date_col: str,\n",
        "                                    n_splits: int,\n",
        "                                    min_train_days: int,\n",
        "                                    val_days: int) -> list[tuple[np.ndarray, np.ndarray]]:\n",
        "        dates = df[date_col].values\n",
        "        unique_dates = np.unique(dates)\n",
        "        result: list[tuple[np.ndarray, np.ndarray]] = []\n",
        "        if len(unique_dates) < (min_train_days + val_days + 1):\n",
        "            return result\n",
        "        anchors = np.linspace(min_train_days, len(unique_dates) - val_days, n_splits, dtype=int)\n",
        "        for a in anchors:\n",
        "            train_last_date = unique_dates[a - 1]\n",
        "            val_last_date = unique_dates[min(a + val_days - 1, len(unique_dates) - 1)]\n",
        "            tr_idx = np.where(dates <= train_last_date)[0]\n",
        "            val_idx = np.where((dates > train_last_date) & (dates <= val_last_date))[0]\n",
        "            if len(val_idx) > 0:\n",
        "                result.append((tr_idx, val_idx))\n",
        "        return result\n",
        "    splits_date = walk_forward_by_dates_local(\n",
        "        train_fe, 'date_id',\n",
        "        n_splits=int(CONFIG.get('cv_n_splits', 5)),\n",
        "        min_train_days=int(CONFIG.get('date_min_train_days', 252)),\n",
        "        val_days=int(CONFIG.get('date_val_days', 120))\n",
        "    )\n",
        "    if len(splits_date):\n",
        "        splits = splits_date\n",
        "        LOGGER.info(f'CV folds (enhanced, date-based): {len(splits)}')\n",
        "    else:\n",
        "        LOGGER.info('Date-based splits not applied (insufficient unique dates); using row-based.')\n",
        "\n",
        "\n",
        "def apply_embargo(tr_idx: np.ndarray, val_idx: np.ndarray, days: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    if days <= 0 or len(val_idx) == 0 or len(tr_idx) == 0:\n",
        "        return tr_idx, val_idx\n",
        "    # Prefer date-based embargo if date_id available\n",
        "    try:\n",
        "        if 'date_id' in train_fe.columns:\n",
        "            dates_all = train_fe['date_id'].values\n",
        "            unique_dates = np.unique(dates_all)\n",
        "            date_to_ord = {d: i for i, d in enumerate(unique_dates)}\n",
        "            tr_ord = np.array([date_to_ord[d] for d in dates_all[tr_idx]], dtype=int)\n",
        "            val_ord = np.array([date_to_ord[d] for d in dates_all[val_idx]], dtype=int)\n",
        "            max_tr_ord = int(tr_ord.max())\n",
        "            min_val_ord = int(val_ord.min())\n",
        "            tr_mask = tr_ord <= (max_tr_ord - days)\n",
        "            val_mask = val_ord >= (min_val_ord + days)\n",
        "            tr_idx2 = tr_idx[tr_mask]\n",
        "            val_idx2 = val_idx[val_mask]\n",
        "            return tr_idx2, val_idx2\n",
        "    except Exception:\n",
        "        pass\n",
        "    # Fallback: index-based embargo\n",
        "    keep_train = max(0, len(tr_idx) - days)\n",
        "    tr_idx2 = tr_idx[:keep_train]\n",
        "    val_idx2 = val_idx[days:] if len(val_idx) > days else np.array([], dtype=int)\n",
        "    return tr_idx2, val_idx2\n",
        "\n",
        "# Train LightGBM with nested calibration on val per fold\n",
        "lgb_params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',  # use RMSE for early stopping; corr only as feval\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.02,\n",
        "    'num_leaves': 64,\n",
        "    'max_depth': -1,\n",
        "    'min_data_in_leaf': 50,\n",
        "    'feature_fraction': 0.7,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'seed': SEED,\n",
        "    'bagging_seed': SEED + 1,\n",
        "    'feature_fraction_seed': SEED + 2,\n",
        "    'verbosity': -1,\n",
        "    'num_threads': -1,\n",
        "}\n",
        "\n",
        "# Optional Optuna tuning across multiple folds with stability penalty\n",
        "if bool(CONFIG.get('enable_optuna', False)) and int(CONFIG.get('optuna_trials', 0)) > 0:\n",
        "    try:\n",
        "        import optuna\n",
        "        LOGGER.info('[Optuna] Starting LightGBM tuning (multi-fold)...')\n",
        "        use_folds = min(int(CONFIG.get('optuna_folds', 1)), len(splits))\n",
        "        folds_for_tune = [splits[i] for i in range(use_folds)]\n",
        "        def build_fold_data(tr_idx, val_idx):\n",
        "            # For tuning, disable embargo to ensure enough validation samples\n",
        "            tr_idx2, val_idx2 = apply_embargo(tr_idx, val_idx, 0)\n",
        "            X_tr0 = train_fe.iloc[tr_idx2][features]\n",
        "            y_tr0 = train_fe.iloc[tr_idx2][TARGET]\n",
        "            X_val0 = train_fe.iloc[val_idx2][features]\n",
        "            y_val0 = train_fe.iloc[val_idx2][TARGET]\n",
        "            w_tr0 = make_time_decay_weights(len(X_tr0), int(CONFIG.get('time_decay_halflife_days', 0)))\n",
        "            return X_tr0, y_tr0, X_val0, y_val0, w_tr0, val_idx2\n",
        "        fold_data_cache = [build_fold_data(*fold) for fold in folds_for_tune]\n",
        "        def objective(trial):\n",
        "            params = lgb_params.copy()\n",
        "            params.update({\n",
        "                'metric': 'l2',  # RMSE for early stopping in tuning\n",
        "                'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
        "                'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.9),\n",
        "                'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.9),\n",
        "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 200),\n",
        "                'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 5.0),\n",
        "                'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 5.0),\n",
        "                'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.0, 1.0),\n",
        "            })\n",
        "            scores = []\n",
        "            for (X_tr0, y_tr0, X_val0, y_val0, w_tr0, val_idx2) in fold_data_cache:\n",
        "                dtrain0 = lgb.Dataset(X_tr0, label=y_tr0, weight=w_tr0)\n",
        "                dvalid0 = lgb.Dataset(X_val0, label=y_val0)\n",
        "                try:\n",
        "                    # Ensure RMSE for early stopping during tuning\n",
        "                    params['metric'] = 'l2'\n",
        "                    bst0 = lgb.train(\n",
        "                        params=params,\n",
        "                        train_set=dtrain0,\n",
        "                        num_boost_round=1000,\n",
        "                        valid_sets=[dtrain0, dvalid0],\n",
        "                        valid_names=['train','valid'],\n",
        "                        callbacks=[\n",
        "                            lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
        "                        ],\n",
        "                        feval=corr_feval if bool(CONFIG.get('use_corr_feval', False)) else None,\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    if isinstance(e, Exception):\n",
        "                        params.pop('device_type', None)\n",
        "                        bst0 = lgb.train(\n",
        "                            params=params,\n",
        "                            train_set=dtrain0,\n",
        "                            num_boost_round=1000,\n",
        "                            valid_sets=[dtrain0, dvalid0],\n",
        "                            valid_names=['train','valid'],\n",
        "                            callbacks=[\n",
        "                                lgb.early_stopping(stopping_rounds=200, verbose=False),\n",
        "                            ],\n",
        "                            feval=corr_feval if bool(CONFIG.get('use_corr_feval', False)) else None,\n",
        "                        )\n",
        "                # Ensure fold has sufficient validation samples\n",
        "                if len(y_val0) < 10:\n",
        "                    scores.append(-1e6)\n",
        "                    continue\n",
        "\n",
        "                val_pred0 = bst0.predict(X_val0, num_iteration=bst0.best_iteration)\n",
        "                val_pred0 = np.nan_to_num(val_pred0, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                # Score by correlation for tuning (avoid brittle calibration here)\n",
        "                score = float(fast_corr(y_val0, val_pred0))\n",
        "\n",
        "                if not np.isfinite(score):\n",
        "                    score = -1e6\n",
        "                scores.append(score)\n",
        "            mean_score = float(np.mean(scores)) if len(scores) else -np.inf\n",
        "            std_score = float(np.std(scores)) if len(scores) > 1 else 0.0\n",
        "            penalty = float(CONFIG.get('optuna_stability_penalty', 0.0)) * std_score\n",
        "            return mean_score - penalty\n",
        "        study = optuna.create_study(direction='maximize')\n",
        "        study.optimize(objective, n_trials=int(CONFIG.get('optuna_trials', 20)), timeout=int(CONFIG.get('optuna_timeout_min', 20))*60)\n",
        "        best_params = study.best_params\n",
        "        LOGGER.info(f\"[Optuna] Best params: {best_params}\")\n",
        "        lgb_params.update(best_params)\n",
        "        aggressive_gc('post-optuna')\n",
        "    except Exception as e:\n",
        "        LOGGER.warning(f'[Optuna] Skipped: {e}')\n",
        "\n",
        "# Optional extra model libraries and base params\n",
        "xgb_available = False\n",
        "cat_available = False\n",
        "if bool(CONFIG.get('enable_xgboost', False)):\n",
        "    try:\n",
        "        import xgboost as xgb\n",
        "        xgb_available = True\n",
        "        xgb_params_base = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'learning_rate': float(lgb_params.get('learning_rate', 0.02)),\n",
        "            'max_depth': 6,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.7,\n",
        "            'lambda': 1.0,\n",
        "            'alpha': 0.0,\n",
        "            'tree_method': 'hist',\n",
        "            'seed': int(SEED),\n",
        "        }\n",
        "        # GPU for XGBoost if available (handle XGB v2+ vs v1.x)\n",
        "        if bool(CONFIG.get('use_gpu_if_available', False)) and 'GPU_AVAILABLE' in globals() and GPU_AVAILABLE:\n",
        "            try:\n",
        "                ver_parts = [int(p) for p in str(getattr(xgb, '__version__', '1.7.0')).split('.')[:2]]\n",
        "            except Exception:\n",
        "                ver_parts = [1, 7]\n",
        "            if ver_parts[0] >= 2:\n",
        "                xgb_params_base.update({'device': 'cuda'})\n",
        "            else:\n",
        "                xgb_params_base.update({'tree_method': 'gpu_hist', 'predictor': 'gpu_predictor', 'gpu_id': 0})\n",
        "            LOGGER.info('[GPU] Enabled XGBoost CUDA acceleration')\n",
        "    except Exception as e:\n",
        "        LOGGER.warning(f'XGBoost unavailable: {e}')\n",
        "if bool(CONFIG.get('enable_catboost', False)):\n",
        "    try:\n",
        "        from catboost import CatBoostRegressor, Pool as CatPool\n",
        "        cat_available = True\n",
        "        cat_params_base = {\n",
        "            'loss_function': 'RMSE',\n",
        "            'learning_rate': float(lgb_params.get('learning_rate', 0.02)),\n",
        "            'depth': 6,\n",
        "            'l2_leaf_reg': 3.0,\n",
        "            'random_seed': int(SEED),\n",
        "            'od_type': 'Iter',\n",
        "            'od_wait': 200,\n",
        "            'thread_count': -1,\n",
        "            'verbose': False,\n",
        "        }\n",
        "        # GPU for CatBoost if available\n",
        "        if bool(CONFIG.get('use_gpu_if_available', False)) and 'GPU_AVAILABLE' in globals() and GPU_AVAILABLE:\n",
        "            # Force GPU 0; CatBoost auto-select may fail on some Kaggle images\n",
        "            cat_params_base.update({'task_type': 'GPU', 'devices': '0'})\n",
        "            LOGGER.info('[GPU] Enabled CatBoost GPU')\n",
        "    except Exception as e:\n",
        "        LOGGER.warning(f'CatBoost unavailable: {e}')\n",
        "\n",
        "models = []\n",
        "# Optional additional model containers\n",
        "models_xgb = []\n",
        "models_cat = []\n",
        "per_model_val_sharpes = {'lgb': [], 'xgb': [], 'cat': []}\n",
        "\n",
        "val_scores = []\n",
        "fold_sharpes = []\n",
        "fold_risk_params = []\n",
        "feature_importance_df = pd.DataFrame()\n",
        "\n",
        "# OOF containers\n",
        "oof_pred = np.zeros(n_samples, dtype=float)\n",
        "oof_counts = np.zeros(n_samples, dtype=float)\n",
        "oof_alloc = np.zeros(n_samples, dtype=float)\n",
        "\n",
        "for fold, (tr_idx, val_idx) in enumerate(splits):\n",
        "    tr_idx, val_idx = apply_embargo(tr_idx, val_idx, int(CONFIG.get('fold_embargo_days', 0)))\n",
        "    if len(val_idx) == 0:\n",
        "        LOGGER.warning(f'Fold {fold+1}: empty val after embargo; skipping')\n",
        "        continue\n",
        "\n",
        "    X_tr = train_fe.iloc[tr_idx][features]\n",
        "    y_tr = train_fe.iloc[tr_idx][TARGET]\n",
        "    X_val = train_fe.iloc[val_idx][features]\n",
        "    y_val = train_fe.iloc[val_idx][TARGET]\n",
        "\n",
        "    LOGGER.info(f'[Enhanced] Fold {fold+1}/{len(splits)} — train {len(tr_idx)} val {len(val_idx)}')\n",
        "    # Time-decay weights (recent data up-weighted)\n",
        "    hl_days = int(CONFIG.get('time_decay_halflife_days', 0))\n",
        "    w_tr = make_time_decay_weights(len(X_tr), hl_days)\n",
        "    # Apply inverse-volatility modifier if realized exists\n",
        "    if 'realized_returns' in train_fe.columns:\n",
        "        realized_tr = train_fe.iloc[tr_idx]['realized_returns']\n",
        "        w_tr = apply_inverse_vol_weights(w_tr, realized_tr, int(CONFIG.get('realized_vol_halflife', 60)))\n",
        "\n",
        "    dtrain = lgb.Dataset(X_tr, label=y_tr, weight=w_tr)\n",
        "    dvalid = lgb.Dataset(X_val, label=y_val)\n",
        "\n",
        "    try:\n",
        "        bst = lgb.train(\n",
        "            params=lgb_params,\n",
        "            train_set=dtrain,\n",
        "            num_boost_round=2000,\n",
        "            valid_sets=[dtrain, dvalid],\n",
        "            valid_names=['train', 'valid'],\n",
        "            callbacks=[\n",
        "                lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
        "                lgb.log_evaluation(period=200),\n",
        "            ],\n",
        "            feval=corr_feval if bool(CONFIG.get('use_corr_feval', False)) else None,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        if isinstance(e, Exception):\n",
        "            lgb_params.pop('device_type', None)\n",
        "            bst = lgb.train(\n",
        "                params=lgb_params,\n",
        "                train_set=dtrain,\n",
        "                num_boost_round=2000,\n",
        "                valid_sets=[dtrain, dvalid],\n",
        "                valid_names=['train', 'valid'],\n",
        "                callbacks=[\n",
        "                    lgb.early_stopping(stopping_rounds=200, verbose=True),\n",
        "                    lgb.log_evaluation(period=200),\n",
        "                ],\n",
        "                feval=corr_feval if bool(CONFIG.get('use_corr_feval', False)) else None,\n",
        "            )\n",
        "\n",
        "    models.append(bst)\n",
        "\n",
        "    val_pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
        "    rmse = mean_squared_error(y_val, val_pred, squared=False)\n",
        "    if bool(CONFIG.get('eval_correlation', False)):\n",
        "        corr = fast_corr(y_val, val_pred)\n",
        "        LOGGER.info(f\"Fold {fold+1} RMSE: {rmse:.6f} | Corr: {corr:.4f}\")\n",
        "    else:\n",
        "        LOGGER.info(f\"Fold {fold+1} RMSE: {rmse:.6f}\")\n",
        "    val_scores.append(rmse)\n",
        "\n",
        "    # Nested calibration using only val period\n",
        "    if 'realized_returns' in train_fe.columns:\n",
        "        realized_val = train_fe.iloc[val_idx]['realized_returns'].astype(float)\n",
        "        best = calibrate_k_and_band(\n",
        "            oof_pred=pd.Series(val_pred, index=realized_val.index),\n",
        "            realized=realized_val,\n",
        "            pred_halflife=int(CONFIG['oof_pred_vol_halflife']),\n",
        "            realized_halflife=int(CONFIG['realized_vol_halflife']),\n",
        "            k_grid=CONFIG['risk_k_grid'],\n",
        "            band_grid=CONFIG['neutral_band_grid'],\n",
        "            vol_cap_multiple=float(CONFIG['vol_cap_multiple']),\n",
        "            turnover_bps=float(CONFIG.get('transaction_cost_bps', 0.0)),\n",
        "        )\n",
        "        fold_risk_params.append(best)\n",
        "        # Map to allocation for OOF alloc series and Sharpe eval\n",
        "        k_use = best.get('k', None)\n",
        "        band_use = best.get('band', None)\n",
        "        if k_use is None or not np.isfinite(k_use):\n",
        "            k_use = 0.8\n",
        "        if band_use is None or not np.isfinite(band_use):\n",
        "            band_use = 0.0\n",
        "        pred_sigma = ewma_std(pd.Series(val_pred, index=realized_val.index), int(CONFIG['oof_pred_vol_halflife']))\n",
        "        z = pd.Series(val_pred, index=realized_val.index) / (pred_sigma.replace(0, np.nan).fillna(1e-6))\n",
        "        z_band = z.where(z.abs() >= band_use, 0.0)\n",
        "        alloc_val = (1.0 + k_use * z_band).clip(0.0, 2.0)\n",
        "        # Vol cap scaling vs market vol\n",
        "        mkt_vol = ewma_std(realized_val, int(CONFIG['realized_vol_halflife']))\n",
        "        strat_vol = ewma_std(alloc_val * realized_val, int(CONFIG['realized_vol_halflife']))\n",
        "        cap = (float(CONFIG['vol_cap_multiple']) * mkt_vol) / (strat_vol.replace(0, np.nan).fillna(1e-6))\n",
        "        cap = cap.clip(0.0, 2.0)\n",
        "        alloc_val = (1.0 + (alloc_val - 1.0) * cap).clip(0.0, 2.0)\n",
        "        strat_ret_val = alloc_val * realized_val\n",
        "        sharpe_val = annualized_sharpe(strat_ret_val.fillna(0.0), days_per_year=int(CONFIG.get('annualization', 252)))\n",
        "        fold_sharpes.append(float(sharpe_val))\n",
        "        per_model_val_sharpes['lgb'].append(float(sharpe_val))\n",
        "        # Store OOF alloc and pred\n",
        "        oof_alloc[val_idx] += alloc_val.values\n",
        "    else:\n",
        "        fold_risk_params.append({'k': None, 'band': None, 'score': None})\n",
        "        fold_sharpes.append(float('nan'))\n",
        "        per_model_val_sharpes['lgb'].append(float('nan'))\n",
        "\n",
        "    oof_pred[val_idx] += val_pred\n",
        "    oof_counts[val_idx] += 1\n",
        "\n",
        "    fi = pd.DataFrame({\n",
        "        'feature': features,\n",
        "        'importance': bst.feature_importance(importance_type='gain'),\n",
        "        'fold': fold,\n",
        "    })\n",
        "    feature_importance_df = pd.concat([feature_importance_df, fi], axis=0)\n",
        "\n",
        "    # Optional: diversified models on same fold\n",
        "    if xgb_available:\n",
        "        try:\n",
        "            dtr = xgb.DMatrix(X_tr, label=y_tr, weight=w_tr)\n",
        "            dvl = xgb.DMatrix(X_val, label=y_val)\n",
        "            xgb_params = xgb_params_base.copy()\n",
        "            xgb_params['seed'] = int(SEED + 31 * fold)\n",
        "            bst_xgb = xgb.train(\n",
        "                params=xgb_params,\n",
        "                dtrain=dtr,\n",
        "                num_boost_round=2000,\n",
        "                evals=[(dtr, 'train'), (dvl, 'valid')],\n",
        "                verbose_eval=False,\n",
        "            )\n",
        "            nlim = getattr(bst_xgb, 'best_ntree_limit', None)\n",
        "            val_pred_xgb = bst_xgb.predict(dvl, ntree_limit=nlim) if nlim else bst_xgb.predict(dvl)\n",
        "            if 'realized_returns' in train_fe.columns:\n",
        "                realized_val2 = train_fe.iloc[val_idx]['realized_returns'].astype(float)\n",
        "                best_x = calibrate_k_and_band(pd.Series(val_pred_xgb, index=realized_val2.index), realized_val2,\n",
        "                                              int(CONFIG['oof_pred_vol_halflife']), int(CONFIG['realized_vol_halflife']),\n",
        "                                              CONFIG['risk_k_grid'], CONFIG['neutral_band_grid'], float(CONFIG['vol_cap_multiple']),\n",
        "                                              turnover_bps=float(CONFIG.get('transaction_cost_bps', 0.0)))\n",
        "                per_model_val_sharpes['xgb'].append(float(best_x['score']))\n",
        "            else:\n",
        "                per_model_val_sharpes['xgb'].append(float(fast_corr(y_val, val_pred_xgb)))\n",
        "            models_xgb.append(bst_xgb)\n",
        "        except Exception as e:\n",
        "            LOGGER.warning(f'XGB fold {fold+1} skipped: {e}')\n",
        "        aggressive_gc(f'post-fold-xgb-{fold+1}')\n",
        "\n",
        "    if cat_available:\n",
        "        try:\n",
        "            model_cat = CatBoostRegressor(**cat_params_base)\n",
        "            model_cat.set_params(random_seed=int(SEED + 37 * fold))\n",
        "            model_cat.fit(X_tr, y_tr, sample_weight=w_tr, eval_set=(X_val, y_val), use_best_model=True, verbose=False)\n",
        "            val_pred_cat = model_cat.predict(X_val)\n",
        "            if 'realized_returns' in train_fe.columns:\n",
        "                realized_val3 = train_fe.iloc[val_idx]['realized_returns'].astype(float)\n",
        "                best_c = calibrate_k_and_band(pd.Series(val_pred_cat, index=realized_val3.index), realized_val3,\n",
        "                                              int(CONFIG['oof_pred_vol_halflife']), int(CONFIG['realized_vol_halflife']),\n",
        "                                              CONFIG['risk_k_grid'], CONFIG['neutral_band_grid'], float(CONFIG['vol_cap_multiple']),\n",
        "                                              turnover_bps=float(CONFIG.get('transaction_cost_bps', 0.0)))\n",
        "                per_model_val_sharpes['cat'].append(float(best_c['score']))\n",
        "            else:\n",
        "                per_model_val_sharpes['cat'].append(float(fast_corr(y_val, val_pred_cat)))\n",
        "            models_cat.append(model_cat)\n",
        "        except Exception as e:\n",
        "            LOGGER.warning(f'CatBoost fold {fold+1} skipped: {e}')\n",
        "        aggressive_gc(f'post-fold-cat-{fold+1}')\n",
        "\n",
        "    del X_tr, y_tr, X_val, y_val, dtrain, dvalid\n",
        "    gc.collect()\n",
        "    aggressive_gc(f'post-fold-{fold+1}')\n",
        "\n",
        "mask = oof_counts > 0\n",
        "oof_pred[mask] /= oof_counts[mask]\n",
        "if 'realized_returns' in train_fe.columns:\n",
        "    oof_alloc[mask] /= oof_counts[mask]\n",
        "\n",
        "train_fe['pred'] = oof_pred\n",
        "if 'realized_returns' in train_fe.columns:\n",
        "    train_fe['alloc_oof'] = np.clip(oof_alloc, 0.0, 2.0)\n",
        "\n",
        "# Optional stacking meta-learner on OOF predictions across model families\n",
        "meta_pred = None\n",
        "if bool(CONFIG.get('use_stacking_meta_learner', False)) and (len(models) or len(models_xgb) or len(models_cat)):\n",
        "    try:\n",
        "        from sklearn.linear_model import Ridge\n",
        "        # Build OOF matrix: for each fold, get predictions from each available family\n",
        "        fam_names = []\n",
        "        oof_matrix = []\n",
        "        # We will rebuild per-fold preds to avoid leakage\n",
        "        for fold, (tr_idx, val_idx) in enumerate(splits):\n",
        "            tr_idx2, val_idx2 = apply_embargo(tr_idx, val_idx, int(CONFIG.get('fold_embargo_days', 0)))\n",
        "            if len(val_idx2) == 0:\n",
        "                continue\n",
        "            X_val = train_fe.iloc[val_idx2][features]\n",
        "            row_preds = []\n",
        "            fam_names_fold = []\n",
        "            if fold < len(models):\n",
        "                row_preds.append(models[fold].predict(X_val, num_iteration=models[fold].best_iteration))\n",
        "                fam_names_fold.append('lgb')\n",
        "            if xgb_available and fold < len(models_xgb):\n",
        "                dvl = xgb.DMatrix(X_val)\n",
        "                nlim = getattr(models_xgb[fold], 'best_ntree_limit', None)\n",
        "                row_preds.append(models_xgb[fold].predict(dvl, ntree_limit=nlim) if nlim else models_xgb[fold].predict(dvl))\n",
        "                fam_names_fold.append('xgb')\n",
        "            if cat_available and fold < len(models_cat):\n",
        "                row_preds.append(models_cat[fold].predict(X_val))\n",
        "                fam_names_fold.append('cat')\n",
        "            if not fam_names:\n",
        "                fam_names = fam_names_fold\n",
        "            if row_preds:\n",
        "                stacked = np.vstack(row_preds).T\n",
        "                oof_matrix.append((val_idx2, stacked))\n",
        "        if oof_matrix:\n",
        "            # Assemble full matrix\n",
        "            n = len(train_fe)\n",
        "            k = len(fam_names)\n",
        "            M = np.full((n, k), np.nan, dtype=float)\n",
        "            for idxs, block in oof_matrix:\n",
        "                M[idxs, :block.shape[1]] = block\n",
        "            mask_rows = ~np.isnan(M).any(axis=1)\n",
        "            M_fit = M[mask_rows]\n",
        "            y_fit = train_fe.loc[mask_rows, TARGET].values\n",
        "            meta = Ridge(alpha=1.0, fit_intercept=True)\n",
        "            meta.fit(M_fit, y_fit)\n",
        "            # meta OOF prediction\n",
        "            meta_pred = np.full(n, np.nan, dtype=float)\n",
        "            meta_pred[mask_rows] = meta.predict(M_fit)\n",
        "            train_fe['pred_meta'] = meta_pred\n",
        "            LOGGER.info(f\"[Stacking] Meta-learner trained with families: {fam_names}\")\n",
        "    except Exception as e:\n",
        "        LOGGER.warning(f'[Stacking] Skipped: {e}')\n",
        "\n",
        "fi_mean = (feature_importance_df.groupby('feature')['importance']\n",
        "           .mean().sort_values(ascending=False))\n",
        "LOGGER.info('[Enhanced] Top 20 features by avg gain:\\n' + str(fi_mean.head(20)))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "fi_mean.head(20).sort_values().plot(kind='barh')\n",
        "plt.title('Top 20 feature importances (enhanced, avg gain)')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG['artifacts_dir'], 'feature_importances_top20_enhanced.png'))\n",
        "plt.close()\n",
        "\n",
        "# Select top-K features for final training if configured, with stability filter\n",
        "_topk = CONFIG.get('final_topk_features', None)\n",
        "min_folds_keep = int(CONFIG.get('stability_filter_min_folds', 0))\n",
        "if not fi_mean.empty and min_folds_keep > 0:\n",
        "    fi_counts = (feature_importance_df.groupby('feature')['fold']\n",
        "                 .nunique().sort_values(ascending=False))\n",
        "    stable_feats = set(fi_counts[fi_counts >= min_folds_keep].index.tolist())\n",
        "else:\n",
        "    stable_feats = set(features)\n",
        "\n",
        "if isinstance(_topk, int) and _topk > 0 and not fi_mean.empty:\n",
        "    selected = list(fi_mean.head(_topk).index)\n",
        "    selected = [f for f in selected if f in stable_feats]\n",
        "    final_features = [f for f in features if f in selected]\n",
        "    LOGGER.info(f\"[Enhanced] Using top-K features with stability filter: K={len(final_features)}\")\n",
        "else:\n",
        "    final_features = [f for f in features if f in stable_feats]\n",
        "\n",
        "# Aggregate risk params across folds (median)\n",
        "if len(fold_risk_params) and all(p.get('k') is not None for p in fold_risk_params):\n",
        "    k_med = float(np.median([p['k'] for p in fold_risk_params]))\n",
        "    band_med = float(np.median([p['band'] for p in fold_risk_params]))\n",
        "    best_risk = {'k': k_med, 'band': band_med, 'score': float(np.nanmean(fold_sharpes))}\n",
        "    LOGGER.info(f\"[Enhanced] Aggregated risk params: k={k_med}, band={band_med}, Sharpe_mean={best_risk['score']:.4f}\")\n",
        "else:\n",
        "    best_risk = {'k': 0.8, 'band': 0.0, 'score': None}\n",
        "\n",
        "# Diagnostics on OOF allocations\n",
        "if 'realized_returns' in train_fe.columns and 'alloc_oof' in train_fe.columns:\n",
        "    tc = CONFIG.get('transaction_cost_bps', 0.0) / 10000.0\n",
        "    alloc_change = pd.Series(train_fe['alloc_oof']).diff().abs().fillna(0.0)\n",
        "    tc_series = tc * alloc_change\n",
        "    strat_oof = (train_fe['alloc_oof'] * train_fe['realized_returns']) - tc_series\n",
        "    sharpe_oof = annualized_sharpe(strat_oof.fillna(0.0), days_per_year=int(CONFIG.get('annualization', 252)))\n",
        "    LOGGER.info(f\"[Enhanced] OOF Sharpe (after TC): {sharpe_oof:.4f}\")\n",
        "\n",
        "# Train final models on full data\n",
        "X_full = train_fe[final_features]\n",
        "y_full = train_fe[TARGET]\n",
        "\n",
        "avg_best_iter = int(np.clip(np.mean([m.best_iteration for m in models]) if len(models) else 1000, 200, 2000))\n",
        "LOGGER.info(f'[Enhanced] Using avg_best_iter: {avg_best_iter}')\n",
        "\n",
        "final_models = []\n",
        "N_FINAL = int(CONFIG.get('final_ensemble_size', 3))\n",
        "for i in range(N_FINAL):\n",
        "    params = lgb_params.copy()\n",
        "    params['seed'] = SEED + 17 * i\n",
        "    params['bagging_seed'] = SEED + 17 * i + 1\n",
        "    params['feature_fraction_seed'] = SEED + 17 * i + 2\n",
        "    # Apply time-decay weights on full data as well (mild)\n",
        "    w_full = make_time_decay_weights(len(X_full), int(CONFIG.get('time_decay_halflife_days', 0)))\n",
        "    dtrain = lgb.Dataset(X_full, label=y_full, weight=w_full)\n",
        "    try:\n",
        "        bst = lgb.train(params, dtrain, num_boost_round=avg_best_iter)\n",
        "    except Exception as e:\n",
        "        if isinstance(e, Exception):\n",
        "            params.pop('device_type', None)\n",
        "            bst = lgb.train(params, dtrain, num_boost_round=avg_best_iter)\n",
        "    final_models.append(bst)\n",
        "    del dtrain\n",
        "\n",
        "# Predict test with rank-based ensembling\n",
        "X_test = test_fe[final_features]\n",
        "preds_list = [bst.predict(X_test, num_iteration=avg_best_iter) for bst in final_models]\n",
        "\n",
        "# Optional smoothing of test predictions before mapping\n",
        "HALF = int(CONFIG.get('pred_smooth_halflife', 0))\n",
        "if HALF and HALF > 0:\n",
        "    def ewma1d(a, halflife):\n",
        "        if len(a) == 0:\n",
        "            return a\n",
        "        lam = np.log(2.0) / max(1.0, float(halflife))\n",
        "        out = np.zeros_like(a, dtype=float)\n",
        "        alpha = 1.0 - np.exp(-lam)\n",
        "        out[0] = a[0]\n",
        "        for i in range(1, len(a)):\n",
        "            out[i] = alpha * a[i] + (1.0 - alpha) * out[i-1]\n",
        "        return out\n",
        "    preds_list = [ewma1d(p, HALF) for p in preds_list]\n",
        "\n",
        "# If stacking meta-learner was trained, include its full-train version\n",
        "if bool(CONFIG.get('use_stacking_meta_learner', False)) and 'pred_meta' in train_fe.columns:\n",
        "    try:\n",
        "        from sklearn.linear_model import Ridge\n",
        "        fam_preds_full = []\n",
        "        fam_names_full = []\n",
        "        # Build family predictions on full train for meta fit, then predict test\n",
        "        # Refit simple Ridge on full data using same families available\n",
        "        # Assemble matrix for train\n",
        "        fam_preds_train = []\n",
        "        if len(final_models):\n",
        "            fam_preds_train.append(np.mean([m.predict(X_full, num_iteration=avg_best_iter) for m in final_models], axis=0))\n",
        "            fam_preds_full.append(np.mean([m.predict(X_test, num_iteration=avg_best_iter) for m in final_models], axis=0))\n",
        "            fam_names_full.append('lgb')\n",
        "        if xgb_available and len(models_xgb):\n",
        "            # Train a new xgb on full (already done below), so use preds_list seeds later\n",
        "            pass\n",
        "        if cat_available and len(models_cat):\n",
        "            pass\n",
        "        if fam_preds_full and fam_preds_train:\n",
        "            M_tr = np.vstack(fam_preds_train).T\n",
        "            y_tr = y_full.values if hasattr(y_full, 'values') else y_full\n",
        "            meta_full = Ridge(alpha=1.0, fit_intercept=True)\n",
        "            meta_full.fit(M_tr, y_tr)\n",
        "            M_te = np.vstack(fam_preds_full).T\n",
        "            preds_list.append(meta_full.predict(M_te))\n",
        "            LOGGER.info('[Stacking] Added meta-learner predictions to ensemble')\n",
        "    except Exception as e:\n",
        "        LOGGER.warning(f'[Stacking] Meta-learner final prediction skipped: {e}')\n",
        "\n",
        "# Add diversified final models if enabled (multi-seed bagging)\n",
        "if xgb_available:\n",
        "    try:\n",
        "        dfull = xgb.DMatrix(X_full, label=y_full)\n",
        "        dtest = xgb.DMatrix(X_test)\n",
        "        n_xgb = int(CONFIG.get('final_ensemble_size_xgb', 1))\n",
        "        for i in range(n_xgb):\n",
        "            xgb_params = xgb_params_base.copy()\n",
        "            xgb_params['seed'] = int(SEED + 101 * i)\n",
        "            xgb_final = xgb.train(params=xgb_params, dtrain=dfull, num_boost_round=avg_best_iter, verbose_eval=False)\n",
        "            nlim2 = getattr(xgb_final, 'best_ntree_limit', None)\n",
        "            preds_list.append(xgb_final.predict(dtest, ntree_limit=nlim2) if nlim2 else xgb_final.predict(dtest))\n",
        "    except Exception as e:\n",
        "        LOGGER.warning(f'XGB final skipped: {e}')\n",
        "    aggressive_gc('post-xgb-final')\n",
        "\n",
        "if cat_available:\n",
        "    try:\n",
        "        n_cat = int(CONFIG.get('final_ensemble_size_cat', 1))\n",
        "        for i in range(n_cat):\n",
        "            cat_params = cat_params_base.copy()\n",
        "            cat_params['random_seed'] = int(SEED + 151 * i)\n",
        "            cat_final = CatBoostRegressor(**cat_params)\n",
        "            cat_final.fit(X_full, y_full, verbose=False)\n",
        "            preds_list.append(cat_final.predict(X_test))\n",
        "    except Exception as e:\n",
        "        LOGGER.warning(f'CatBoost final skipped: {e}')\n",
        "    aggressive_gc('post-cat-final')\n",
        "\n",
        "# Weighted rank-averaging based on validation Sharpe if available\n",
        "weights = None\n",
        "if str(CONFIG.get('ensemble_weighting', 'equal')).lower() == 'sharpe' and len(per_model_val_sharpes['lgb']) == len(splits):\n",
        "    w_lgb = max(1e-6, float(np.nanmean(per_model_val_sharpes['lgb'])))\n",
        "    w_xgb = max(1e-6, float(np.nanmean(per_model_val_sharpes['xgb']))) if xgb_available and len(per_model_val_sharpes['xgb']) else 0.0\n",
        "    w_cat = max(1e-6, float(np.nanmean(per_model_val_sharpes['cat']))) if cat_available and len(per_model_val_sharpes['cat']) else 0.0\n",
        "    # Each family weight is spread equally across its seeds\n",
        "    weights = [w_lgb / max(len(final_models), 1.0)] * len(final_models)\n",
        "    if xgb_available:\n",
        "        n_xgb = int(CONFIG.get('final_ensemble_size_xgb', 1))\n",
        "        weights.extend([w_xgb / max(n_xgb, 1.0)] * n_xgb)\n",
        "    if cat_available:\n",
        "        n_cat = int(CONFIG.get('final_ensemble_size_cat', 1))\n",
        "        weights.extend([w_cat / max(n_cat, 1.0)] * n_cat)\n",
        "\n",
        "if weights is None:\n",
        "    weights = [1.0] * len(preds_list)\n",
        "\n",
        "# Non-negative least squares (NNLS) on ranks as a fallback to Sharpe-weights if desired\n",
        "# We prefer stability; keep default to Sharpe weights unless CONFIG requests nnls\n",
        "if str(CONFIG.get('ensemble_weighting', 'sharpe')).lower() == 'nnls':\n",
        "    try:\n",
        "        from scipy.optimize import nnls\n",
        "        ranked = [rank_normalize(p) for p in preds_list]\n",
        "        ranked_stack = np.stack(ranked, axis=1)\n",
        "        # target vector: mean of ranked OOF predictions proxy (use train ranks if available)\n",
        "        # use equal target to encourage smooth combination\n",
        "        y_target = np.mean(ranked_stack, axis=1)\n",
        "        w_nnls, _ = nnls(ranked_stack, y_target)\n",
        "        if np.isfinite(w_nnls).all() and w_nnls.sum() > 0:\n",
        "            weights_arr = w_nnls / w_nnls.sum()\n",
        "        else:\n",
        "            weights_arr = np.asarray(weights, dtype=float) / (np.sum(weights) + 1e-12)\n",
        "    except Exception:\n",
        "        weights_arr = np.asarray(weights, dtype=float) / (np.sum(weights) + 1e-12)\n",
        "else:\n",
        "    weights_arr = np.asarray(weights, dtype=float) / (np.sum(weights) + 1e-12)\n",
        "\n",
        "# Align weights to number of prediction columns\n",
        "weights_arr = np.asarray(weights_arr, dtype=float)\n",
        "if weights_arr.shape[0] != len(preds_list):\n",
        "    LOGGER.warning(f'Adjusting ensemble weights from {weights_arr.shape[0]} to {len(preds_list)} to match models')\n",
        "    if weights_arr.shape[0] > 0:\n",
        "        weights_arr = np.resize(weights_arr, len(preds_list))\n",
        "        weights_arr = weights_arr / (weights_arr.sum() + 1e-12)\n",
        "    else:\n",
        "        weights_arr = np.ones(len(preds_list), dtype=float) / max(len(preds_list), 1)\n",
        "\n",
        "ranked = [rank_normalize(p) for p in preds_list]\n",
        "ranked_stack = np.stack(ranked, axis=1)\n",
        "ranked_preds = np.dot(ranked_stack, weights_arr)\n",
        "\n",
        "preds_test = ranked_preds - 0.5\n",
        "\n",
        "LOGGER.info('[Enhanced] Built enhanced pipeline with nested calibration, diversified models, and weighted ensemble predictions.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Date-based CV helper and validation checks ===\n",
        "from typing import Iterator, Tuple\n",
        "\n",
        "def assert_required_columns(df: pd.DataFrame, cols: list):\n",
        "    missing = [c for c in cols if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "# Ensure time order and integer monotonicity for date_id if present\n",
        "if 'date_id' in train.columns:\n",
        "    assert train['date_id'].is_monotonic_increasing, 'train.date_id must be sorted ascending'\n",
        "if 'date_id' in test.columns:\n",
        "    assert test['date_id'].is_monotonic_increasing, 'test.date_id must be sorted ascending'\n",
        "\n",
        "assert_required_columns(train_fe, features + [TARGET])\n",
        "\n",
        "# Optionally allow date-aware split sizes (fixed by unique dates rather than rows)\n",
        "def walk_forward_by_dates(df: pd.DataFrame,\n",
        "                          date_col: str,\n",
        "                          n_splits: int = 5,\n",
        "                          min_train_days: int = 252,\n",
        "                          val_days: int = 120) -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
        "    dates = df[date_col].values\n",
        "    unique_dates = np.unique(dates)\n",
        "    if len(unique_dates) < (min_train_days + val_days + 1):\n",
        "        # fallback to row-based splits already built\n",
        "        for s in splits:\n",
        "            yield s\n",
        "        return\n",
        "    anchors = np.linspace(min_train_days, len(unique_dates) - val_days, n_splits, dtype=int)\n",
        "    for a in anchors:\n",
        "        train_last_date = unique_dates[a - 1]\n",
        "        val_last_date = unique_dates[min(a + val_days - 1, len(unique_dates) - 1)]\n",
        "        tr_idx = np.where(dates <= train_last_date)[0]\n",
        "        val_idx = np.where((dates > train_last_date) & (dates <= val_last_date))[0]\n",
        "        if len(val_idx) > 0:\n",
        "            yield tr_idx, val_idx\n",
        "\n",
        "# If date_id exists, we can produce alternative date-based splits for diagnostics\n",
        "if 'date_id' in train_fe.columns:\n",
        "    date_splits = list(walk_forward_by_dates(train_fe, 'date_id'))\n",
        "    LOGGER.info(f\"Date-based splits prepared: {len(date_splits)} folds\")\n",
        "else:\n",
        "    date_splits = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Calibrated train diagnostics using aggregated risk params (no duplicate utils) ===\n",
        "\n",
        "if 'pred' in train_fe.columns and realized_returns is not None:\n",
        "    # Use aggregated best_risk from enhanced CV if available; else fallback\n",
        "    k_use = best_risk['k'] if 'best_risk' in globals() and best_risk.get('k') is not None else 0.8\n",
        "    band_use = best_risk['band'] if 'best_risk' in globals() and best_risk.get('band') is not None else 0.0\n",
        "\n",
        "    pred_sigma = ewma_std(train_fe['pred'], int(CONFIG['oof_pred_vol_halflife']))\n",
        "    z = train_fe['pred'] / (pred_sigma.replace(0, np.nan).fillna(1e-6))\n",
        "    z_band = z.where(z.abs() >= band_use, 0.0)\n",
        "    alloc_calib = (1.0 + k_use * z_band).clip(0.0, 2.0)\n",
        "\n",
        "    mkt_vol = ewma_std(train_fe['realized_returns'], int(CONFIG['realized_vol_halflife']))\n",
        "    strat_vol = ewma_std(alloc_calib * train_fe['realized_returns'], int(CONFIG['realized_vol_halflife']))\n",
        "    cap = (float(CONFIG['vol_cap_multiple']) * mkt_vol) / (strat_vol.replace(0, np.nan).fillna(1e-6))\n",
        "    cap = cap.clip(0.0, 2.0)\n",
        "    train_fe['alloc_calibrated'] = (1.0 + (alloc_calib - 1.0) * cap).clip(0.0, 2.0)\n",
        "\n",
        "    # Apply transaction costs on calibrated series\n",
        "    tc = CONFIG.get('transaction_cost_bps', 0.0) / 10000.0\n",
        "    alloc_change_cal = train_fe['alloc_calibrated'].diff().abs().fillna(0.0)\n",
        "    tc_cal = tc * alloc_change_cal\n",
        "\n",
        "    # Diagnostics\n",
        "    strat_cal = (train_fe['alloc_calibrated'] * train_fe['realized_returns']) - tc_cal\n",
        "    cum_cal = (1.0 + strat_cal).cumprod()\n",
        "    cum_mkt = (1.0 + train_fe['realized_returns']).cumprod()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    x_axis = train_fe['date_id'] if 'date_id' in train_fe.columns else np.arange(len(train_fe))\n",
        "    plt.plot(x_axis, cum_mkt, label='Market (buy-hold)')\n",
        "    plt.plot(x_axis, cum_cal, label='Strategy (calibrated)')\n",
        "    plt.legend(); plt.title('Cumulative performance — calibrated (enhanced)'); plt.tight_layout()\n",
        "    plt.savefig(os.path.join(CONFIG['artifacts_dir'], 'backtest_cumulative_calibrated_enhanced.png'))\n",
        "    plt.close()\n",
        "\n",
        "    sharpe_cal = annualized_sharpe(strat_cal.fillna(0.0), days_per_year=int(CONFIG.get('annualization', 252)))\n",
        "    LOGGER.info(f\"[Enhanced] Calibrated train Sharpe (after TC): {sharpe_cal:.4f}\")\n",
        "else:\n",
        "    LOGGER.warning('Skipping calibrated diagnostics: missing pred or realized_returns.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Apply calibrated risk to TEST predictions (if available) and save artifacts ===\n",
        "\n",
        "# If we calibrated best_risk, we also map test predictions with the same parameters.\n",
        "# We reuse test predictions already computed (ranked_preds -> preds_test) and\n",
        "# estimate sigma from recent train OOF prediction EWMA.\n",
        "\n",
        "artifacts = {}\n",
        "\n",
        "if 'pred' in train_fe.columns:\n",
        "    pred_sigma_train = ewma_std(train_fe['pred'], int(CONFIG['oof_pred_vol_halflife']))\n",
        "    sigma_est = float(pred_sigma_train.iloc[-50:].median()) if pred_sigma_train.notna().any() else float(train_fe['pred'].std())\n",
        "else:\n",
        "    sigma_est = float(train_fe['pred'].std()) if 'pred' in train_fe else 1.0\n",
        "\n",
        "# Prefer aggregated params if present\n",
        "k_use = best_risk['k'] if 'best_risk' in globals() and best_risk.get('k') is not None else 0.8\n",
        "band_use = best_risk['band'] if 'best_risk' in globals() and best_risk.get('band') is not None else 0.0\n",
        "\n",
        "# Center preds_test (already centered earlier). Apply band and scaling.\n",
        "z_test = preds_test / (sigma_est + 1e-9)\n",
        "z_test_band = np.where(np.abs(z_test) >= band_use, z_test, 0.0)\n",
        "alloc_test_cal = 1.0 + k_use * z_test_band\n",
        "alloc_test_cal = np.clip(alloc_test_cal, 0.0, 2.0)\n",
        "\n",
        "# Optional global downscale using recent historical cap (after costs)\n",
        "if realized_returns is not None and 'alloc_calibrated' in train_fe.columns:\n",
        "    hist_window = min(len(train_fe), 180)\n",
        "    tc = CONFIG.get('transaction_cost_bps', 0.0) / 10000.0\n",
        "    alloc_change_hist = train_fe['alloc_calibrated'].iloc[-hist_window:].diff().abs().fillna(0.0)\n",
        "    tc_hist = tc * alloc_change_hist\n",
        "    strat_hist = (train_fe['alloc_calibrated'].iloc[-hist_window:] * train_fe['realized_returns'].iloc[-hist_window:]) - tc_hist\n",
        "    hist_strat_vol = float(ewma_std(strat_hist, int(CONFIG['realized_vol_halflife'])).iloc[-1]) if len(strat_hist) else float(strat_hist.std())\n",
        "    hist_mkt_vol   = float(ewma_std(train_fe['realized_returns'].iloc[-hist_window:], int(CONFIG['realized_vol_halflife'])).iloc[-1])\n",
        "    scale_global = min(1.0, (float(CONFIG['vol_cap_multiple']) * hist_mkt_vol) / (hist_strat_vol + 1e-9)) if hist_strat_vol > 0 else 1.0\n",
        "    alloc_test_cal = 1.0 + (alloc_test_cal - 1.0) * scale_global\n",
        "    alloc_test_cal = np.clip(alloc_test_cal, 0.0, 2.0)\n",
        "\n",
        "# Final single submission write (calibrated allocations)\n",
        "submission_cal = pd.DataFrame({\n",
        "    'date_id': test_fe['date_id'].values if 'date_id' in test_fe.columns else np.arange(len(test_fe)),\n",
        "    'allocation': alloc_test_cal.astype(float)\n",
        "})\n",
        "submission_cal.to_csv('submission.csv', index=False)\n",
        "LOGGER.info(f\"Wrote submission.csv with {submission_cal.shape[0]} rows (calibrated)\")\n",
        "\n",
        "# Save artifacts for reproducibility\n",
        "ensure_dir(CONFIG['artifacts_dir'])\n",
        "\n",
        "# CV metrics (+ Sharpe)\n",
        "cv_metrics = {\n",
        "    'cv_rmse_mean': float(np.mean(val_scores)) if len(val_scores) else None,\n",
        "    'cv_rmse_std': float(np.std(val_scores)) if len(val_scores) else None,\n",
        "    'cv_sharpe_mean': float(np.nanmean(fold_sharpes)) if 'fold_sharpes' in globals() and len(fold_sharpes) else None,\n",
        "    'cv_sharpe_median': float(np.nanmedian(fold_sharpes)) if 'fold_sharpes' in globals() and len(fold_sharpes) else None,\n",
        "    'n_models_lgb': int(len(models)),\n",
        "    'n_models_xgb': int(len(models_xgb)) if 'models_xgb' in globals() else 0,\n",
        "    'n_models_cat': int(len(models_cat)) if 'models_cat' in globals() else 0,\n",
        "    'ensemble_weighting': CONFIG.get('ensemble_weighting', 'equal'),\n",
        "}\n",
        "with open(os.path.join(CONFIG['artifacts_dir'], 'cv_metrics.json'), 'w') as f:\n",
        "    json.dump(cv_metrics, f, indent=2)\n",
        "\n",
        "# Fold risk params\n",
        "if 'fold_risk_params' in globals() and len(fold_risk_params):\n",
        "    with open(os.path.join(CONFIG['artifacts_dir'], 'fold_risk_params.json'), 'w') as f:\n",
        "        json.dump(fold_risk_params, f, indent=2)\n",
        "\n",
        "# OOF predictions\n",
        "if 'pred' in train_fe.columns:\n",
        "    cols = ['date_id', TARGET, 'pred'] if 'date_id' in train_fe.columns else [TARGET, 'pred']\n",
        "    train_fe[cols].to_csv(os.path.join(CONFIG['artifacts_dir'], 'oof_predictions.csv'), index=False)\n",
        "\n",
        "# Feature importances\n",
        "if 'fi_mean' in globals() and not fi_mean.empty:\n",
        "    fi_mean.reset_index().rename(columns={'index':'feature','importance':'importance_mean'}) \\\n",
        "        .to_csv(os.path.join(CONFIG['artifacts_dir'], 'feature_importances.csv'), index=False)\n",
        "\n",
        "# Save features list\n",
        "with open(os.path.join(CONFIG['artifacts_dir'], 'features.json'), 'w') as f:\n",
        "    json.dump(sorted(list(features)), f, indent=2)\n",
        "\n",
        "# Save final models\n",
        "try:\n",
        "    import joblib\n",
        "    joblib.dump(final_models, os.path.join(CONFIG['artifacts_dir'], 'final_lgb_models.pkl'))\n",
        "    LOGGER.info('Saved final_lgb_models.pkl to artifacts/')\n",
        "except Exception as e:\n",
        "    LOGGER.warning(f'Model save skipped: {e}')\n",
        "\n",
        "# Metadata (include calibrated risk if available)\n",
        "# Ensemble weights snapshot (if used)\n",
        "try:\n",
        "    ens_weights = None\n",
        "    if 'weights_arr' in globals():\n",
        "        ens_weights = list(map(float, weights_arr.tolist()))\n",
        "except Exception:\n",
        "    ens_weights = None\n",
        "\n",
        "metadata = {\n",
        "    'config': CONFIG,\n",
        "    'avg_best_iter': int(avg_best_iter),\n",
        "    'seed': int(SEED),\n",
        "    'best_risk': best_risk if 'best_risk' in globals() else None,\n",
        "    'ensemble_weights': ens_weights,\n",
        "}\n",
        "with open(os.path.join(CONFIG['artifacts_dir'], 'metadata.json'), 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# Total runtime log\n",
        "elapsed = time.time() - start_time\n",
        "LOGGER.info(f'Total runtime: {elapsed/60:.1f} min')\n",
        "\n",
        "LOGGER.info('Artifacts saved to artifacts/.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
